<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Alejandro Pardo</title>
  <link rel="stylesheet" href="styles.css">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P89SCSHN5F"></script>
  <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
          dataLayer.push(arguments);
      }
      gtag('js', new Date());
      gtag('config', 'G-P89SCSHN5F'); // Replace with your Measurement ID
  </script>
</head>
<body>
  <header class="header">
    <div class="header-content">
      <img src="assets/me.png" alt="Profile Photo" class="profile-photo">
      <div class="intro-text">
        <h1>Alejandro Pardo</h1>
        <p>
          I'm a final-year Ph.D. student at <a href="https://kaust.edu.sa" target="_blank">KAUST</a> under the supervision of 
          Professor <a href="https://www.bernardghanem.com" target="_blank">Bernard Ghanem</a>. Previously, I completed my M.Sc. degree advised by <a href="https://scholar.google.com/citations?user=k0nZO90AAAAJ&hl=en" target="_blank">Pablo Arbelaez</a>. 
          During my PhD, I interned at the Embodied AI Labs at Intel and Adobe Research.
        </p>
        <p>
          Currently, my research focuses on leveraging modern Computer Vision algorithms to automate <b>creative video editing</b>, trying to bridge the gap between creativity and technology. 
          If you share similar interests, feel free to reach out—I’d love to connect and exchange ideas!
        </p>
        <p><b>I am actively looking for permanent positions!</b></p>
        <p>Email: alejandro dot pardo at kaust dot edu dot sa</p>
        <div class="links">
          <a href="assets/CV.pdf" target="_blank">CV</a> / 
          <a href="https://scholar.google.com/citations?user=_lKVc3sAAAAJ&hl=en&oi=ao">Scholar</a> / 
          <a href="https://www.linkedin.com/in/luis-alejandro-pardo-gonzalez/">Linkedin</a> 
        </div>
      </div>
    </div>
  </header>
  
  <main class="main">
    <section class="research">
      <h2>Featured Research</h2>
      <p>
        Here are some of my most representative works. For a complete list of publications, feel free to visit my 
        <a href="https://scholar.google.com/citations?user=_lKVc3sAAAAJ&hl=en&oi=ao">Google Scholar</a> page. 
        My research spans various video understanding topics that might also interest you.
      </p>
      
      <article class="research-item">
        <div class="media">
          <img src="assets/leaf_butterfly.gif" alt="MatchDiffusion Image">
        </div>
        <div class="content">
          <h3><a href="https://matchdiffusion.github.io">MatchDiffusion: Training-free Generation of Match-Cuts</a></h3>
          <p><b>Alejandro Pardo*</b>, Fabio Pizzati*, Tong Zhang, Alexander Pondaven, Philip Torr, Juan Camilo Perez, Bernard Ghanem<br>
          <i>Under Review</i> - <a href="https://matchdiffusion.github.io">Project Page</a> / <a href="https://arxiv.org/abs/2411.18677">arXiv</a> / <a href="https://github.com/PardoAlejo/MatchDiffusion">Code</a></p>
          <p>
            We introduce a training-free method for generating match-cuts using text-to-video diffusion models. 
            By leveraging the denoising process, our approach creates visually coherent video pairs with shared structure but distinct semantics, enabling the creation of seamless and impactful transitions. 
          </p>
        </div>
      </article>
      
      <article class="research-item">
        <div class="media">
          <img src="assets/mushroom_operation_only.gif" alt="Assembler Image">
        </div>
        <div class="content">
          <h3><a href="https://sites.google.com/kaust.edu.sa/timeline-assembler">Generative Timelines for Instructed Visual Assembly</a></h3>
          <p><b>Alejandro Pardo</b>,  Jui-Hsien Wang,  Bernard Ghanem,  Josef Sivic, Bryan Russell, Fabian Caba Heilbron<br>
            <i>NeurIPS Workshop on Video-Language Models</i> - <a href="https://sites.google.com/kaust.edu.sa/timeline-assembler">Website</a> / <a href="https://arxiv.org/abs/2411.12293">arXiv</a></p>
            <p>*Work done during Internship at Adobe Resarch</p>
          <p>
            We introduce the Timeline Assembler, a generative model that enables intuitive visual timeline editing using natural language instructions. 
            Our method automates complex tasks like reordering, adding, and removing clips, making video editing accessible to non-experts. 
          </p>
        </div>
      </article>
      
      <article class="research-item">
        <div class="media">
          <img src="assets/clms.png" alt="CLMs Image">
        </div>
        <div class="content">
          <h3><a href="https://arxiv.org/abs/2405.17146">Compressed-Language Mmodels for Uunderstanding Compressed Formats:a JPEG exploration</a></h3>
          <p>Juan C. Pérez, <b>Alejandro Pardo</b>,  Mattia Soldan,  Hani Itani, Juan Leon-Alcázar, Bernard Ghanem<br>
            <i>Under Review</i> -  / <a href="https://arxiv.org/abs/2405.17146">arXiv</a></p>
          <p>
            This work explores the potential of Compressed-Language Models (CLMs) to process and understand data directly from compressed file formats (CFFs) like JPEG. 
            By treating compressed byte streams as sequences, we evaluate CLMs across recognizing file properties, handling anomalies, and generating new files. 
            Our findings reveal that CLMs can effectively grasp the semantics of compressed data, showcasing the promise of directly leveraging compressed formats for efficient and universal data processing. 
          </p>
        </div>
      </article>

      <article class="research-item">
        <div class="media">
          <img src="assets/TGT.png" alt="TGT Image">
        </div>
        <div class="content">
          <h3><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Towards_Automated_Movie_Trailer_Generation_CVPR_2024_paper.pdf">Towards Automated Movie Trailer Generation</a></h3>
          <p>Dawit Mureja Argaw, Mattia Soldan, <b>Alejandro Pardo</b>,  Chen Zhao,  Fabian Caba Heilbron, Joon Son Chung, Bernard Ghanem<br>
          <i>CVPR-2024</i> - <a href="">GitHub</a> / <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Argaw_Towards_Automated_Movie_Trailer_Generation_CVPR_2024_paper.pdf">Paper</a></p>
          <p>
            This work presents an approach to automate trailer creation using the Trailer Generation Transformer (TGT), a sequence-to-sequence model designed to predict plausible movie trailers. 
            By leveraging an encoder-decoder architecture, TGT models the temporal order and relevance of movie shots to create engaging trailers. 
            Our method overcomes the limitations of prior classification and ranking-based approaches, achieving state-of-the-art results on newly curated benchmarks for automatic trailer generation.
          </p>
        </div>
      </article>

      <article class="research-item">
        <div class="media">
          <img src="assets/moviecuts.png" alt="MovieCuts Image">
        </div>
        <div class="content">
          <h3><a href="https://arxiv.org/abs/2109.05569">MovieCuts: A New Dataset and Benchmark for Cut Type Recognition</a></h3>
          <p><b>Alejandro Pardo</b>,  Fabian Caba Heilbron,  Juan Leon-Alcázar, Ali Thabet, Bernard Ghanem<br>
          <i>ECCV-2022</i> - <a href="https://github.com/PardoAlejo/MovieCuts">GitHub</a> / <a href="https://arxiv.org/abs/2109.05569">arXiv</a></p>
          <p>
            We present MovieCuts, a large-scale dataset and benchmark for recognizing cinematic cut types. 
            With over 173,000 clips labeled with ten professional cut categories, MovieCuts addresses the multi-modal challenges of analyzing audio-visual transitions. 
            Our benchmarks highlight the complexity of this task, paving the way for advancements in automated video editing, virtual cinematography, and film education. 
          </p>
        </div>
      </article>


      <article class="research-item">
        <div class="media">
          <img src="assets/vis_mad-1.png" alt="MAD Image">
        </div>
        <div class="content">
          <h3><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Soldan_MAD_A_Scalable_Dataset_for_Language_Grounding_in_Videos_From_CVPR_2022_paper.pdf">MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions</a></h3>
          <p>Mattia Soldan, <b>Alejandro Pardo</b>,  Juan Leon-Alcázar,  Fabian Caba Heilbron, Chen Zhao, Silvio Giancola, Bernard Ghanem<br>
          <i>CVPR-2022</i> - <a href="https://github.com/Soldelli/MAD">GitHub</a> / <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Soldan_MAD_A_Scalable_Dataset_for_Language_Grounding_in_Videos_From_CVPR_2022_paper.pdf">Paper</a></p>
          <p>We introduce MAD (Movie Audio Descriptions), a large-scale dataset for video-language grounding with over 384,000 descriptive sentences aligned to 1,200+ hours of long-form movies. 
            By leveraging professional audio descriptions, MAD reduces biases seen in prior datasets and challenges models to temporally ground short language moments in diverse, untrimmed videos. 
            This benchmark pushes the boundaries of video-language research and practical applications like smart video search and editing.
          </p>
        </div>
      </article>

    </section>
  </main>
  
  
  <footer class="footer">
    <div class="footer-content">
      <p>© 2024 Alejandro Pardo</p>
    </div>
  </footer>
</body>
</html>
